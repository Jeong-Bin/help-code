1. 배깅 원리

 1) 각 모델의 다양성을 어떻게 확보할 것인가?
    - 전체 데이터셋에서 랜덤 샘플링(복원추출)을 진행하여 여러 데이터셋을 생성
    - 각 데이터셋으로 모델을 만듦
    - 모델별로 학습하는 데이터셋의 다양성으로 인해 모델의 다양성이 확보됨

 2) 최종 결과물을 어떻게 결합할 것인가?
    - 각 모델들로부터 나온 예측치의 평균을 낸다.

배깅을 활용한 대표적 모델 : <랜덤 포레스트>
의사결정나무로 여러가지 예측치를 내고 평균을 내는 방법

<랜덤 포레스트> - 파라미터
    (1) n_estimators : 몇 개의 의사결정나무를 만들 것인지. 보통 100~500개
    (2) min_samples_split : 의사결정 나무에서 각 구간의 최소 샘플 수.
                            이것보다 작으면 더이상 분리되지 않음




2. 부스팅 원리

 1) 각 모델의 다양성을 어떻게 확보할 것인가?
    - 이전 모델에서 오분류한 객체에 가중치를 높여서 새로운 데이터로 모델 학습
    - 각 데이터셋으로 모델을 만듦
    - 모델별로 학습하는 데이터셋의 다양성으로 인해 모델의 다양성이 확보됨

 2) 최종 결과물을 어떻게 결합할 것인가?
    - 각 모델로부터 나온 예측치의 가중평균을 낸다.

부스팅을 활용한 대표적 모델 : <XG 부스트>
마찬가지로 의사결정나무를 이용

<XG 부스트> - 파라미터
    (1) n_estimators : 몇 개의 의사결정나무를 만들 것인지.
    (2) learning_rate : 얼마나 빠르게 학습할 것인지(가중치를 얼마나 극단적으로 업데이트시킬 것인지)
                        보통 0.05 ~ 0.1로 설정
                        
                        
* n_estimators가 너무 높으면 노이즈에 민감한 오버피팅의 우려가 있다.
* n_estimators가 너무 작으면 언더피팅의 우려가 있다. - 지역적특성을 제대로 반영 못함


<XG 부스트> - 프로세스
    (1) 일반적으로 learning_rate를 0.05 혹은 0.1로 고정시키고 n_estimators에 변형을 줘 모델을 학습시킨다.
    (2) 만들어진 모델을 이용해 검증 데이터에서의 에러율을 확인한다.
    (3) 에러율이 가장 작은 n_estimators가 최적의 n_estimators다.